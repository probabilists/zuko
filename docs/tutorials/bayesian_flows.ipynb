{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee0e7ec-9429-42d0-bb62-e37ea7161702",
   "metadata": {},
   "source": [
    "# Bayesian Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a8493-95d5-47e1-b1b0-c7323a06d979",
   "metadata": {},
   "source": [
    "Bayesian networks are similar to normal Linear models, where each weight is replaced by a learnable gaussian distribution. \n",
    "\n",
    "At each evaluation, a bayesian model performs a sampling of its weights values and returns a different result. \n",
    "\n",
    "During the training the average and std of the weights is learnt to reproduce the target posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b53fc-3d55-45a7-9e7e-98c73729f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from torch import nn\n",
    "\n",
    "import zuko"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e70fa68-3fd9-458d-8ccf-5945a00c5d10",
   "metadata": {},
   "source": [
    "Each model can be made \"bayesian\" by using the `BayesianModel` wrapper from zuko. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd3df0-3eed-438a-90c2-cae2f7bd4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zuko.bayesian import BayesianModel\n",
    "\n",
    "adjacency = torch.randn(4, 3) < 0\n",
    "net = zuko.nn.MaskedMLP(adjacency, [16, 32], activation=nn.ELU)\n",
    "\n",
    "# Create a Bayesian version of the network\n",
    "# init_logvar controls the initial uncertainty of the weights\n",
    "bnet = BayesianModel(net, init_logvar=-3.0)\n",
    "bnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419ace7",
   "metadata": {},
   "source": [
    "The bayesian model needs to be \"sampled\" before using it. \n",
    "There are two ways of using the model:\n",
    "- create a sampled model instance: this creates a copy of the original model, by replacing the Linear layers weights with samples from the bayesian model. This model does not propagate the gradients to the original model's parameters, so it must not be used for training.\n",
    "- a **context manager**: this mode does not create a copy of the model but it replaces on the fly the forward method of the linear layer in order to sample the weights from the bayesian model. This is **the recommended** way of using the Bayesian model as it is much more memory efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbaa71-f9c0-4f23-85ec-eadbad4b3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_model = bnet.sample_model()\n",
    "\n",
    "x = torch.randn(3)\n",
    "jac = torch.autograd.functional.jacobian(sampled_model, x)\n",
    "jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ce2ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bnet.reparameterize() as sampled_model:\n",
    "    x = torch.randn(3)\n",
    "    print(torch.autograd.functional.jacobian(sampled_model, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9db63c-bb40-4242-b013-c706813835de",
   "metadata": {},
   "source": [
    "At each evaluation the weights of the bayesian MLP are sampled from their gaussian distribution, producing a different value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f6346-3f07-4694-b402-6e2c0c8b01b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    with bnet.reparameterize() as sampled_model:\n",
    "        print(sampled_model(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4733bd5-33c9-43bc-bdfc-38c1ff7f7bfe",
   "metadata": {},
   "source": [
    "## Creating Bayesian flows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8ffd3",
   "metadata": {},
   "source": [
    "Any flow can be converted to a Bayesian flow. Each parameter will be converted to a gaussian distribution of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2051a-fbff-452b-91d6-6c25eb879695",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = zuko.flows.spline.NSF(\n",
    "    features=3, context=5, bins=10, transforms=3, hidden_features=[64, 64]\n",
    ")\n",
    "\n",
    "bmodel = BayesianModel(model, init_logvar=-8.0)\n",
    "bmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a781b1-7e21-4337-841b-9868a9c9b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand((10, 5))\n",
    "x = torch.rand((10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c47b7-215f-4abc-8d8a-eb97b4e7140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    with bmodel.reparameterize() as model:\n",
    "        print(model(c).log_prob(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af32b7-f963-4488-9eca-c8fe27fbfe2f",
   "metadata": {},
   "source": [
    "The log probability estimated with a baysian flow is a distribution,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7136efd1-43d6-4dfe-ba8f-f8a442754d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = []\n",
    "for i in range(100):\n",
    "    with bmodel.reparameterize() as model:\n",
    "        o.append(model(c).log_prob(x)[0])\n",
    "\n",
    "plt.hist(torch.stack(o).squeeze().cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d2cac",
   "metadata": {},
   "source": [
    "Parameters can be explicitely included or excluded using patter-matching expressions while defining the bayesian model. For example, to make only the last layer of the each transformation bayesian one can use this expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7613aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = zuko.flows.spline.NSF(\n",
    "    features=3, context=5, bins=10, transforms=3, hidden_features=[64, 64]\n",
    ")\n",
    "\n",
    "bmodel = BayesianModel(model, init_logvar=-8.0, include_params=[\"transform.transforms.*.hyper.4\"])\n",
    "bmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7522a57",
   "metadata": {},
   "source": [
    "To exclude all the bias parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d906585",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = zuko.flows.spline.NSF(\n",
    "    features=3, context=5, bins=10, transforms=3, hidden_features=[64, 64]\n",
    ")\n",
    "\n",
    "bmodel = BayesianModel(model, init_logvar=-8.0, exclude_params=[\"**.bias\"])\n",
    "bmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba718a7-ed2d-4f71-8d43-ea4461c19431",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26499a1a-8c25-498a-9c72-1e009f91580f",
   "metadata": {},
   "source": [
    "Let's learn a bayesian flow over the classical moon example. We want to evaluate the uncertainty assigned by the flow to the density estimation in each point of the phasespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c9fa5e-1c7c-4c12-a273-f920e68e5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the two-moon dataset\n",
    "Nevents = 500000\n",
    "X, y = make_moons(n_samples=Nevents, noise=0.2, random_state=42)\n",
    "\n",
    "# Separate the data points by class\n",
    "X_class0 = X[y == 0]\n",
    "X_class1 = X[y == 1]\n",
    "plt.scatter(X_class0[:, 0], X_class0[:, 1])\n",
    "plt.scatter(X_class1[:, 0], X_class1[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3527d-01ad-4267-b1e5-bb31869d6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(X).to(torch.float32)\n",
    "c = torch.from_numpy(y).to(torch.float32).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72872d3-067a-4e05-96f0-4342d9fb3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = zuko.flows.spline.NSF(\n",
    "    features=2,\n",
    "    context=1,\n",
    "    bins=5,\n",
    "    transforms=2,\n",
    "    hidden_features=[32, 32],\n",
    ")\n",
    "\n",
    "bmodel = BayesianModel(model, init_logvar=-9.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e777bcd-da58-4fcc-b8e5-6dfd7afd6d11",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aaa0f7-4049-4518-abe0-78a677d7e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(bmodel.parameters(), lr=1e-4)\n",
    "batch_size = 256\n",
    "nepochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c937940-6e06-4e45-9aba-1feadbf13ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log = []\n",
    "indices = np.arange(Nevents)\n",
    "for e in range(nepochs):\n",
    "    np.random.shuffle(indices)\n",
    "    for i in range(Nevents // batch_size):\n",
    "        x_i = x[indices[i * batch_size : (i + 1) * batch_size]]\n",
    "        c_i = c[indices[i * batch_size : (i + 1) * batch_size]]\n",
    "        # print(x.shape)\n",
    "        with bmodel.reparameterize() as smodel:\n",
    "            # sample the model\n",
    "            flow_loss = -smodel(c_i).log_prob(x_i).mean()\n",
    "\n",
    "        # The KL divergence is automatically computed for all\n",
    "        # Bayesian layers in the model when calling this method\n",
    "        kl_loss = bmodel.kl_divergence(prior_var=1.0) / Nevents\n",
    "        loss = flow_loss + kl_loss\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        log.append((loss.item(), flow_loss.item(), kl_loss.item()))\n",
    "        if i % 500 == 0:\n",
    "            print(\n",
    "                f\"epoch={e}, step={i}, total loss: {loss.item():.3f}, flow loss:{flow_loss.item():.3f}, KL loss: {kl_loss.item():.3f}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4e8b9-0db5-42ec-a094-ef6e1010929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[0] for i in log], label=\"total loss\")\n",
    "plt.plot([i[1] for i in log], label=\"flow loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52347550-272b-49a1-9cb9-b78c452de21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i[2] for i in log], label=\"KL loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9716b-20f9-4215-b0c8-f142d408aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with bmodel.reparameterize() as smodel:\n",
    "        samples_1 = smodel(torch.ones((20000, 1))).sample((1,)).cpu().squeeze().numpy()\n",
    "        samples_2 = smodel(torch.zeros((20000, 1))).sample((1,)).cpu().squeeze().numpy()\n",
    "\n",
    "plt.scatter(samples_1[:, 0], samples_1[:, 1])\n",
    "plt.scatter(samples_2[:, 0], samples_2[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840a765-03cd-4116-b7b0-61b54515cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def profiled_histogram_2d(\n",
    "    data_x,\n",
    "    data_y,\n",
    "    values,\n",
    "    bins=10,\n",
    "    range=None,\n",
    "    title=\"2D Profiled Histogram\",\n",
    "    xlabel=\"X Bins\",\n",
    "    ylabel=\"Y Bins\",\n",
    "    cbar_label=\"Average Value\",\n",
    "    norm=None,\n",
    "    vmax=2,\n",
    "):\n",
    "    # Calculate the 2D histogram with weights\n",
    "    counts, x_edges, y_edges = np.histogram2d(\n",
    "        data_x, data_y, bins=bins, range=range, weights=values\n",
    "    )\n",
    "\n",
    "    # Calculate the unweighted 2D histogram to handle empty bins correctly\n",
    "    counts_unweighted, _, _ = np.histogram2d(data_x, data_y, bins=bins, range=range)\n",
    "\n",
    "    # Calculate the mean value for each bin, handling empty bins\n",
    "    bin_means = np.where(counts_unweighted > 0, counts / counts_unweighted, np.nan)\n",
    "\n",
    "    # Plot the 2D profiled histogram\n",
    "    # Use pcolormesh to create the 2D grid of bins with colors representing the mean values\n",
    "    mesh = plt.pcolormesh(\n",
    "        x_edges,\n",
    "        y_edges,\n",
    "        bin_means.T,  # Note the transpose (.T)\n",
    "        cmap=\"viridis\",  # You can choose a different colormap\n",
    "        norm=matplotlib.colors.Normalize(vmin=np.nanmin(bin_means), vmax=vmax),\n",
    "    )  # set the color scale\n",
    "    plt.colorbar(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774dda2-afeb-41bd-8176-c9e7b09a5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel.eval()\n",
    "\n",
    "x_test = x[(c == 1).squeeze(-1)][0:100000]\n",
    "c_test = torch.ones((x_test.shape[0], 1))\n",
    "\n",
    "densities = []\n",
    "with torch.no_grad():\n",
    "    for i in range(30):\n",
    "        with bmodel.reparameterize() as smodel:\n",
    "            densities.append(smodel(c_test).log_prob(x_test))\n",
    "\n",
    "D = torch.stack(densities, dim=1)\n",
    "D_mean = D.mean(dim=1)\n",
    "D_std = D.exp().std(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae3293-529a-497b-aa5e-27d2ee6e8cb8",
   "metadata": {},
   "source": [
    "Average probability density over the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51fdfd-cac5-4ac0-8e77-37ac7d64e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_histogram_2d(\n",
    "    x_test[:, 0].cpu().numpy(),\n",
    "    x_test[:, 1].cpu().numpy(),\n",
    "    D_mean.exp().cpu().numpy(),\n",
    "    bins=100,\n",
    "    vmax=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3df4e3-01dd-4dc8-b8aa-4f79e417fbc9",
   "metadata": {},
   "source": [
    "Average probability density uncertainty over the space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbec490-1925-494f-a11f-4f5ff0701a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_histogram_2d(\n",
    "    x_test[:, 0].cpu().numpy(), x_test[:, 1].cpu().numpy(), D_std.cpu().numpy(), bins=100, vmax=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538c244-453e-4eed-9d66-abd372feb265",
   "metadata": {},
   "source": [
    "Average relative uncertainty of the probability density in the phasespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dc3cda-2afc-4502-a0a9-8bad5a33c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiled_histogram_2d(\n",
    "    x_test[:, 0].cpu().numpy(),\n",
    "    x_test[:, 1].cpu().numpy(),\n",
    "    D_std.cpu().numpy() / D_mean.exp().cpu().numpy(),\n",
    "    bins=100,\n",
    "    vmax=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f7a16-8f1c-4fbe-a53a-8e5d9b0f012f",
   "metadata": {},
   "source": [
    "# Looking at the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c46e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe9f45-aabd-4be3-b520-e24510d3539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logsvars = []\n",
    "for n, p in bmodel.logvars.items():\n",
    "    logsvars.append(p.detach().cpu().numpy().flatten())\n",
    "\n",
    "logsvars = np.concatenate(logsvars, axis=0)\n",
    "logsvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a402804-ccca-4022-b8b2-6ddaf1c17ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(logsvars, bins=50)\n",
    "plt.xlabel(\"Variance of the weight distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d186465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
